%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Privacy Preserving Neural Network Modelling in Insurance using Horizontal Federated Learning}
\date{May 15, 2024}
\release{1.0}
\author{IFoA Federated Learning Working Party: \\ Malgorzata Smietanka (Chair), \\Dylan Liew (Deputy), \\Scott Hand, \\ Harry Lohhy}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxstepscope


\chapter{code}
\label{\detokenize{modules:code}}\label{\detokenize{modules::doc}}
\sphinxstepscope


\section{architecture module}
\label{\detokenize{architecture:module-architecture}}\label{\detokenize{architecture:architecture-module}}\label{\detokenize{architecture::doc}}\index{module@\spxentry{module}!architecture@\spxentry{architecture}}\index{architecture@\spxentry{architecture}!module@\spxentry{module}}\index{MultipleRegression (class in architecture)@\spxentry{MultipleRegression}\spxextra{class in architecture}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{architecture:architecture.MultipleRegression}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w,w}{  }}}\sphinxcode{\sphinxupquote{architecture.}}\sphinxbfcode{\sphinxupquote{MultipleRegression}}}{\sphinxparam{\DUrole{n,n}{num\_features=39}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{num\_units\_1=25}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{num\_units\_2=2}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{activation=\textless{}class \textquotesingle{}torch.nn.modules.activation.Tanh\textquotesingle{}\textgreater{}}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{dropout\_rate=0}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{Module}}

\sphinxAtStartPar
A neural network model for multiple regression with two hidden layers.
\begin{description}
\sphinxlineitem{Attributes:}\begin{description}
\sphinxlineitem{layer\_1}{[}nn.Linear{]}
\sphinxAtStartPar
The first linear layer.

\sphinxlineitem{layer\_2}{[}nn.Linear {]}
\sphinxAtStartPar
The second linear layer.

\sphinxlineitem{layer\_out}{[}nn.Linear {]}
\sphinxAtStartPar
The output layer.

\sphinxlineitem{dropout}{[}nn.Dropout {]}
\sphinxAtStartPar
Dropout layer for regularization.

\sphinxlineitem{act}{[}activation{]}
\sphinxAtStartPar
The activation function.

\end{description}

\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{num\_features}{[}int{]}
\sphinxAtStartPar
Number of input features. Default is 39.

\sphinxlineitem{num\_units\_1}{[}int {]}
\sphinxAtStartPar
Number of units in the first hidden layer. Default is 25.

\sphinxlineitem{num\_units\_2}{[}int{]}
\sphinxAtStartPar
Number of units in the second hidden layer. Default is 2.

\sphinxlineitem{activation}{[}callable {]}
\sphinxAtStartPar
Activation function to use. Default is nn.Tanh.

\sphinxlineitem{dropout\_rate}{[}float{]}
\sphinxAtStartPar
Dropout rate. Default is 0.

\end{description}

\sphinxlineitem{Methods:}\begin{description}
\sphinxlineitem{forward(inputs) }
\sphinxAtStartPar
Performs a forward pass through the model.

\sphinxlineitem{predict(test\_inputs)}
\sphinxAtStartPar
Predicts outputs for the given inputs.

\end{description}

\end{description}
\index{forward() (architecture.MultipleRegression method)@\spxentry{forward()}\spxextra{architecture.MultipleRegression method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{architecture:architecture.MultipleRegression.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\sphinxparam{\DUrole{n,n}{inputs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Performs a forward pass through the model.
\begin{description}
\sphinxlineitem{Parameters:}
\sphinxAtStartPar
inputs (Tensor): Input tensor.

\sphinxlineitem{Returns:}
\sphinxAtStartPar
Tensor: The model’s output tensor.

\end{description}

\end{fulllineitems}

\index{predict() (architecture.MultipleRegression method)@\spxentry{predict()}\spxextra{architecture.MultipleRegression method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{architecture:architecture.MultipleRegression.predict}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\sphinxparam{\DUrole{n,n}{test\_inputs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Predicts outputs for the given inputs without applying dropout.
\begin{description}
\sphinxlineitem{Parameters:}
\sphinxAtStartPar
test\_inputs (Tensor): Input tensor for prediction.

\sphinxlineitem{Returns:}
\sphinxAtStartPar
Tensor: The predicted output tensor.

\end{description}

\end{fulllineitems}

\index{training (architecture.MultipleRegression attribute)@\spxentry{training}\spxextra{architecture.MultipleRegression attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{architecture:architecture.MultipleRegression.training}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{training}}\sphinxbfcode{\sphinxupquote{\DUrole{p,p}{:}\DUrole{w,w}{  }bool}}}
\pysigstopsignatures
\end{fulllineitems}


\end{fulllineitems}

\index{get\_parameters() (in module architecture)@\spxentry{get\_parameters()}\spxextra{in module architecture}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{architecture:architecture.get_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{architecture.}}\sphinxbfcode{\sphinxupquote{get\_parameters}}}{\sphinxparam{\DUrole{n,n}{model}}}{{ $\rightarrow$ List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Retrieves the parameters of the given model as a list of numpy arrays.
\begin{description}
\sphinxlineitem{Parameters:}
\sphinxAtStartPar
model (nn.Module): The model from which to retrieve parameters.

\sphinxlineitem{Returns:}
\sphinxAtStartPar
List{[}np.ndarray{]}: A list containing the model’s parameters as numpy arrays.

\end{description}

\end{fulllineitems}


\sphinxstepscope


\section{insur\_FL\_client module}
\label{\detokenize{insur_FL_client:module-insur_FL_client}}\label{\detokenize{insur_FL_client:insur-fl-client-module}}\label{\detokenize{insur_FL_client::doc}}\index{module@\spxentry{module}!insur\_FL\_client@\spxentry{insur\_FL\_client}}\index{insur\_FL\_client@\spxentry{insur\_FL\_client}!module@\spxentry{module}}\index{ClaimsFrequencyFLClient (class in insur\_FL\_client)@\spxentry{ClaimsFrequencyFLClient}\spxextra{class in insur\_FL\_client}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.ClaimsFrequencyFLClient}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w,w}{  }}}\sphinxcode{\sphinxupquote{insur\_FL\_client.}}\sphinxbfcode{\sphinxupquote{ClaimsFrequencyFLClient}}}{\sphinxparam{\DUrole{o,o}{*}\DUrole{n,n}{args}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{Any}}\sphinxparamcomma \sphinxparam{\DUrole{o,o}{**}\DUrole{n,n}{kwargs}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{Any}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{NumPyClient}}

\sphinxAtStartPar
A client class for federated learning using Flower framework, designed to handle the training and evaluation 
of a machine learning model on local data and interact with a federated learning server.
\begin{description}
\sphinxlineitem{Attributes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
model (torch.nn.Module): The local machine learning model.

\item {} 
\sphinxAtStartPar
optimizer (torch.optim.Optimizer): The optimizer used for training the model.

\item {} 
\sphinxAtStartPar
criterion (torch.nn.Module): The loss function used for model training.

\item {} 
\sphinxAtStartPar
trainset (torch.utils.data.Dataset): The training dataset.

\item {} 
\sphinxAtStartPar
valset (torch.utils.data.Dataset): The validation dataset.

\item {} 
\sphinxAtStartPar
testset (torch.utils.data.Dataset): The test dataset.

\item {} 
\sphinxAtStartPar
num\_examples (Dict): A dictionary containing the number of examples.

\item {} 
\sphinxAtStartPar
exposure (float): A parameter used to weight the parameter updates.

\item {} 
\sphinxAtStartPar
noise (List): A list of noise values added to model parameters for secure aggregation.

\end{itemize}

\end{description}
\index{evaluate() (insur\_FL\_client.ClaimsFrequencyFLClient method)@\spxentry{evaluate()}\spxextra{insur\_FL\_client.ClaimsFrequencyFLClient method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.ClaimsFrequencyFLClient.evaluate}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{evaluate}}}{\sphinxparam{\DUrole{n,n}{parameters}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{config}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{Dict\DUrole{p,p}{{[}}str\DUrole{p,p}{,}\DUrole{w,w}{  }str\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ Tuple\DUrole{p,p}{{[}}float\DUrole{p,p}{,}\DUrole{w,w}{  }int\DUrole{p,p}{,}\DUrole{w,w}{  }Dict\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Evaluates the model with the provided global parameters on local test data.

\sphinxAtStartPar
This method is intended to be used in a federated learning context where global model parameters are evaluated
on a client’s local test dataset. The method sets the model’s parameters to the provided global parameters, evaluates
these parameters on the local test dataset, and returns the evaluation loss, the number of test samples, and a dictionary
containing evaluation metrics such as accuracy.
\begin{description}
\sphinxlineitem{Parameters:}\begin{itemize}
\item {} \begin{description}
\sphinxlineitem{parameters}{[}List{[}np.ndarray{]}{]}
\sphinxAtStartPar
A list of NumPy ndarrays representing the global model parameters to be evaluated.

\end{description}

\item {} \begin{description}
\sphinxlineitem{config}{[}Dict{[}str, str{]}{]}
\sphinxAtStartPar
A dictionary containing configuration options for the evaluation process. This could include model\sphinxhyphen{}specific settings or evaluation hyperparameters. Note: Currently, this parameter is not directly used in the method but is included for consistency and future extensions.

\end{description}

\end{itemize}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{Tuple{[}float, int, Dict{]}}\begin{description}
\sphinxlineitem{A tuple containing three elements:}\begin{itemize}
\item {} 
\sphinxAtStartPar
The evaluation loss as a float.

\item {} 
\sphinxAtStartPar
The number of samples in the test dataset as an int.

\item {} 
\sphinxAtStartPar
A dictionary containing evaluation metrics, with at least an “accuracy” key providing the accuracy of the model on the test dataset calculated as the loss divided by the number of test loader batches.

\end{itemize}

\end{description}

\end{description}

\sphinxlineitem{Examples:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{client\PYGZus{}evaluator} \PYG{o}{=} \PYG{n}{ModelEvaluator}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{criterion}\PYG{p}{,} \PYG{n}{testset}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{global\PYGZus{}params} \PYG{o}{=} \PYG{p}{[}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} Global parameters obtained from the server}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loss}\PYG{p}{,} \PYG{n}{num\PYGZus{}samples}\PYG{p}{,} \PYG{n}{metrics} \PYG{o}{=} \PYG{n}{client\PYGZus{}evaluator}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{n}{global\PYGZus{}params}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Test Loss: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{loss}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Test Accuracy: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accuracy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
The method utilizes a DataLoader to iterate through the test dataset, and the batch size for the DataLoader is determined by the global BATCH\_SIZE variable.

\item {} 
\sphinxAtStartPar
The accuracy calculation in the returned dictionary is a simplified example. Depending on the model and the task, you might need a more sophisticated method to calculate accuracy or other relevant metrics.

\item {} 
\sphinxAtStartPar
Ensure that the global \sphinxtitleref{BATCH\_SIZE} variable is appropriately set for the evaluation DataLoader to function correctly.

\end{itemize}

\end{description}

\end{fulllineitems}

\index{fit() (insur\_FL\_client.ClaimsFrequencyFLClient method)@\spxentry{fit()}\spxextra{insur\_FL\_client.ClaimsFrequencyFLClient method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.ClaimsFrequencyFLClient.fit}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{\sphinxparam{\DUrole{n,n}{parameters}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{config}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{Dict\DUrole{p,p}{{[}}str\DUrole{p,p}{,}\DUrole{w,w}{  }str\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ Tuple\DUrole{p,p}{{[}}List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{]}}\DUrole{p,p}{,}\DUrole{w,w}{  }int\DUrole{p,p}{,}\DUrole{w,w}{  }Dict\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Trains the model locally using provided parameters and configuration settings.

\sphinxAtStartPar
This method sets the initial model parameters, trains the model on a local dataset,
and returns the updated model parameters after training. It encapsulates the process of
local training within a federated learning framework, including setting initial parameters,
executing the training loop, and optionally evaluating the model on a validation set.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{parameters}{[}List{[}np.ndarray{]}{]}
\sphinxAtStartPar
A list of NumPy ndarrays representing the model parameters to be set before training begins.
These parameters might come from a central server in a federated learning setup.

\sphinxlineitem{config}{[}Dict{[}str, str{]}{]}
\sphinxAtStartPar
A dictionary containing configuration options for the training process. This may include
hyperparameters such as learning rate, batch size, or any other model\sphinxhyphen{}specific settings.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{Tuple{[}List{[}np.ndarray{]}, int, Dict{]}}\begin{description}
\sphinxlineitem{A tuple containing three elements:}\begin{itemize}
\item {} 
\sphinxAtStartPar
A list of NumPy ndarrays representing the updated model parameters after training.

\item {} 
\sphinxAtStartPar
An integer representing the number of training samples used in the training process. This could be used for weighted averaging in a federated learning setup.

\item {} 
\sphinxAtStartPar
A dictionary containing additional information about the training process. For example, it could include metrics such as training loss or accuracy, or model\sphinxhyphen{}specific metrics like ‘exposure’ in this case.

\end{itemize}

\end{description}

\end{description}

\sphinxlineitem{Examples:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model\PYGZus{}trainer} \PYG{o}{=} \PYG{n}{ModelTrainer}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{optimizer}\PYG{p}{,} \PYG{n}{criterion}\PYG{p}{,} \PYG{n}{trainset}\PYG{p}{,} \PYG{n}{valset}\PYG{p}{,} \PYG{n}{testset}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{updated\PYGZus{}params}\PYG{p}{,} \PYG{n}{num\PYGZus{}samples}\PYG{p}{,} \PYG{n}{metrics} \PYG{o}{=} \PYG{n}{model\PYGZus{}trainer}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{initial\PYGZus{}params}\PYG{p}{,} \PYG{n}{config}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{metrics}\PYG{p}{)}
\PYG{g+go}{\PYGZob{}\PYGZsq{}exposure\PYGZsq{}: ...\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
The training process uses the DataLoader from PyTorch to load the training and validation datasets with the specified batch size. It’s important to ensure that the datasets are properly initialized and passed to the ModelTrainer before calling \sphinxtitleref{fit}.

\item {} 
\sphinxAtStartPar
The configuration dictionary must include all necessary settings required by the training and evaluation process. Missing configurations might result in default values being used or in runtime errors.

\item {} 
\sphinxAtStartPar
The method internally calls \sphinxtitleref{set\_parameters} to set the model’s initial parameters and \sphinxtitleref{get\_parameters} to retrieve the updated parameters after training. Ensure that these methods are implemented correctly for the \sphinxtitleref{fit} method to work as expected.

\item {} 
\sphinxAtStartPar
This method appends the training statistics to an internal list \sphinxtitleref{self.stats} after each training session, allowing for tracking of performance over multiple rounds of federated learning.

\end{itemize}

\end{description}

\end{fulllineitems}

\index{get\_parameters() (insur\_FL\_client.ClaimsFrequencyFLClient method)@\spxentry{get\_parameters()}\spxextra{insur\_FL\_client.ClaimsFrequencyFLClient method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.ClaimsFrequencyFLClient.get_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_parameters}}}{\sphinxparam{\DUrole{n,n}{config}}}{{ $\rightarrow$ List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Retrieves local model parameters, with optional quantization and noise addition based on configuration flags.

\sphinxAtStartPar
This method extracts the model’s parameters as numpy arrays. If the QUANTISATION flag is set in the provided
configuration, the parameters are quantized. Similarly, if the SMPC\_NOISE flag is set, noise is added to the
parameters. This is part of preparing the model’s parameters for secure multi\sphinxhyphen{}party computation (SMPC) or
other privacy\sphinxhyphen{}preserving mechanisms.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{config}{[}dict  {]}
\sphinxAtStartPar
A configuration dictionary that may contain flags like QUANTISATION and SMPC\_NOISE to indicate whether quantization or noise addition should be applied to the model parameters.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{List{[}np.ndarray{]}}
\sphinxAtStartPar
A list of numpy arrays representing the model’s parameters after applying quantization and/or noise addition as specified in the configuration. Each numpy array in the list corresponds to parameters of a different layer or component of the model.

\end{description}

\sphinxlineitem{Examples:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model} \PYG{o}{=} \PYG{n}{YourModelClass}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{config} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{QUANTISATION}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SMPC\PYGZus{}NOISE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{False}\PYG{p}{\PYGZcb{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{parameters} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}parameters}\PYG{p}{(}\PYG{n}{config}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{parameters}\PYG{p}{)}
\PYG{g+go}{\PYGZlt{}class \PYGZsq{}list\PYGZsq{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{parameters}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{g+go}{\PYGZlt{}class \PYGZsq{}numpy.ndarray\PYGZsq{}\PYGZgt{}}
\end{sphinxVerbatim}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
QUANTISATION and SMPC\_NOISE are flags handling quantization and noise addition.

\end{itemize}

\end{description}

\end{fulllineitems}

\index{set\_parameters() (insur\_FL\_client.ClaimsFrequencyFLClient method)@\spxentry{set\_parameters()}\spxextra{insur\_FL\_client.ClaimsFrequencyFLClient method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.ClaimsFrequencyFLClient.set_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_parameters}}}{\sphinxparam{\DUrole{n,n}{parameters}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ None}}
\pysigstopsignatures
\sphinxAtStartPar
Updates the model’s parameters with new values provided as a list of NumPy ndarrays.

\sphinxAtStartPar
This method takes a list of NumPy arrays containing new parameter values and updates the model’s
parameters accordingly. It’s typically used to set model parameters after they have been modified
or updated elsewhere, possibly after aggregation in a federated learning scenario or after receiving
updates from an optimization process.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{parameters}{[}List{[}np.ndarray{]}{]}
\sphinxAtStartPar
A list of NumPy ndarrays where each array corresponds to the parameters for a different layer or
component of the model. The order of the arrays in the list should match the order of parameters
in the model’s state\_dict.

\end{description}

\sphinxlineitem{Returns:}
\sphinxAtStartPar
None

\sphinxlineitem{Examples:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model} \PYG{o}{=} \PYG{n}{YourModelClass}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{new\PYGZus{}parameters} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model}\PYG{o}{.}\PYG{n}{set\PYGZus{}parameters}\PYG{p}{(}\PYG{n}{new\PYGZus{}parameters}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{} Model parameters are now updated with `new\PYGZus{}parameters`.}
\end{sphinxVerbatim}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
This method assumes that the provided list of parameters matches the structure and order of the model’s parameters. If the order or structure of \sphinxtitleref{parameters} does not match, this may lead to incorrect assignment of parameters or runtime errors.

\item {} 
\sphinxAtStartPar
The method converts each NumPy ndarray to a PyTorch tensor before updating the model’s state dict. Ensure that the data types and device (CPU/GPU) of the NumPy arrays are compatible with your model’s requirements.

\end{itemize}

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{initialize\_model() (in module insur\_FL\_client)@\spxentry{initialize\_model()}\spxextra{in module insur\_FL\_client}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.initialize_model}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_FL\_client.}}\sphinxbfcode{\sphinxupquote{initialize\_model}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes and returns the machine learning model along with the optimizer and loss criterion.

\sphinxAtStartPar
This function initializes a multiple regression model with architecture specified by 
\sphinxtitleref{archit.MultipleRegression}. The model configuration (number of features and units in the first and second layer)
is read from \sphinxtitleref{run\_config}. The model is moved to a device (e.g., CPU or GPU) specified by the global \sphinxtitleref{device} variable.
The optimizer used is NAdam, and the loss criterion is Poisson negative log likelihood.
\begin{description}
\sphinxlineitem{Note:}\begin{itemize}
\item {} 
\sphinxAtStartPar
The function relies on global variables \sphinxtitleref{device}, \sphinxtitleref{run\_config}, \sphinxtitleref{archit}, \sphinxtitleref{optim}, and \sphinxtitleref{nn} 
for its operation.

\item {} 
\sphinxAtStartPar
\sphinxtitleref{run\_config} should have \sphinxtitleref{NUM\_FEATURES}, \sphinxtitleref{NUM\_UNITS\_1}, and \sphinxtitleref{NUM\_UNITS\_2} attributes defined.

\item {} 
\sphinxAtStartPar
\sphinxtitleref{device} should be defined globally and indicate where the model should be allocated (e.g., ‘cuda’ or ‘cpu’).

\end{itemize}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{tuple: A tuple containing three elements:}\begin{itemize}
\item {} 
\sphinxAtStartPar
model (archit.MultipleRegression): The initialized multiple regression model.

\item {} 
\sphinxAtStartPar
optimizer (optim.NAdam): The optimizer for the model.

\item {} 
\sphinxAtStartPar
criterion (nn.PoissonNLLLoss): The loss criterion.

\end{itemize}

\end{description}

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model}\PYG{p}{,} \PYG{n}{optimizer}\PYG{p}{,} \PYG{n}{criterion} \PYG{o}{=} \PYG{n}{initialize\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{\PYGZlt{}class \PYGZsq{}archit.MultipleRegression\PYGZsq{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{optimizer}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{\PYGZlt{}class \PYGZsq{}torch.optim.nadam.NAdam\PYGZsq{}\PYGZgt{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{criterion}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{\PYGZlt{}class \PYGZsq{}torch.nn.modules.loss.PoissonNLLLoss\PYGZsq{}\PYGZgt{}}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}

\index{main() (in module insur\_FL\_client)@\spxentry{main()}\spxextra{in module insur\_FL\_client}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.main}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_FL\_client.}}\sphinxbfcode{\sphinxupquote{main}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Main function to execute the training process.

\sphinxAtStartPar
This function begins by parsing command\sphinxhyphen{}line arguments to configure the training session. It then initializes the random seeds for reproducibility and processes the specified client’s dataset. Based on the provided arguments, it either proceeds with federated learning or exits if centralized training is indicated.

\sphinxAtStartPar
Federated learning involves initializing a model, optimizer, and loss criterion, followed by applying noise to the training process for privacy preservation. The model is then trained with data from the specified client, communicating with a federated learning server as configured.

\sphinxAtStartPar
Finally, the trained model’s state is saved, and training loss statistics are written to a file.
\begin{description}
\sphinxlineitem{Returns:}
\sphinxAtStartPar
int: 0, indicating successful completion of the training process.

\sphinxlineitem{Note:}
\sphinxAtStartPar
This function relies on external configurations from \sphinxtitleref{run\_config}, utility functions from \sphinxtitleref{utils}, and the \sphinxtitleref{ClaimsFrequencyFLClient} for the FL training setup. It assumes the presence of a federated learning server listening on the specified address.

\sphinxlineitem{Example usage:}
\sphinxAtStartPar
To run this script, ensure that the command\sphinxhyphen{}line arguments are correctly set for the desired training configuration. For example:
\sphinxtitleref{python3 insur\_FL\_client.py \textendash{}agent\_id 1 \textendash{}if\_FL 1}
This would initiate federated learning for client with agent\_id 1.

\end{description}

\end{fulllineitems}

\index{parse\_args() (in module insur\_FL\_client)@\spxentry{parse\_args()}\spxextra{in module insur\_FL\_client}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.parse_args}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_FL\_client.}}\sphinxbfcode{\sphinxupquote{parse\_args}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Parses command\sphinxhyphen{}line arguments for training configuration.

\sphinxAtStartPar
This function uses \sphinxtitleref{argparse} to define and parse command\sphinxhyphen{}line arguments necessary for specifying the training mode (centralized or federated learning), the number of federated learning participants, and the partition of data to use for training.
\begin{description}
\sphinxlineitem{Returns:}
\sphinxAtStartPar
Namespace: An argparse.Namespace object containing the arguments and their values. 
\sphinxhyphen{} \sphinxtitleref{agent\_id} (int): Specifies the partition of data for training. A value of \sphinxhyphen{}1 indicates that all data should be used ( training a global model ). Valid values are in the range {[}\sphinxhyphen{}1, run\_config.server\_config{[}“num\_clients”{]}{]}.
\sphinxhyphen{} \sphinxtitleref{if\_FL} (int): Determines the pipeline type. A value of 0 sets the training to centralized (currently disabled), while a value of 1 sets it to federated learning.

\sphinxlineitem{Example command line usage:}
\sphinxAtStartPar
For federated learning with all data:
\sphinxtitleref{python script.py \textendash{}agent\_id \sphinxhyphen{}1 \textendash{}if\_FL 1}

\sphinxAtStartPar
For federated learning for agent 3:
\sphinxtitleref{python script.py \textendash{}agent\_id 3 \textendash{}if\_FL 1}

\end{description}

\end{fulllineitems}

\index{test() (in module insur\_FL\_client)@\spxentry{test()}\spxextra{in module insur\_FL\_client}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.test}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_FL\_client.}}\sphinxbfcode{\sphinxupquote{test}}}{\sphinxparam{\DUrole{n,n}{model}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{criterion}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{val\_loader}}}{{ $\rightarrow$ float}}
\pysigstopsignatures
\sphinxAtStartPar
Evaluates the performance of the model on a validation dataset.

\sphinxAtStartPar
This function iterates over the provided validation DataLoader, computes the loss of the model predictions
against the true labels using the provided loss criterion, and sums up the loss over all validation batches
to get the total validation loss. The model is set to evaluation mode during this process to disable dropout
or batch normalization layers that behave differently during training.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{model}{[}torch.nn.Module{]}
\sphinxAtStartPar
The neural network model to be evaluated. It should already be trained or loaded with pre\sphinxhyphen{}trained weights.

\sphinxlineitem{criterion}{[}torch.nn.Module{]}
\sphinxAtStartPar
The loss function used to calculate the loss between the model predictions and the true labels.

\sphinxlineitem{val\_loader}{[}torch.utils.data.DataLoader{]}
\sphinxAtStartPar
A DataLoader providing batches of validation data including features and labels.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{float}
\sphinxAtStartPar
The total loss computed over all batches of the validation dataset.

\end{description}

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model} \PYG{o}{=} \PYG{n}{MyCustomModel}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{criterion} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{MSELoss}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{val\PYGZus{}dataset} \PYG{o}{=} \PYG{n}{CustomDataset}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{val\PYGZus{}loader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}\PYG{n}{val\PYGZus{}dataset}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{64}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{total\PYGZus{}val\PYGZus{}loss} \PYG{o}{=} \PYG{n}{test}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{criterion}\PYG{p}{,} \PYG{n}{val\PYGZus{}loader}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Total Validation Loss: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{total\PYGZus{}val\PYGZus{}loss}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
The function automatically moves the input and target data to the same device as the model before making predictions.

\end{itemize}

\sphinxAtStartPar
Ensure that the model and criterion are already moved to the appropriate device (CPU or GPU) before calling this function.
\sphinxhyphen{} The function uses \sphinxtitleref{torch.no\_grad()} context manager to disable gradient computation during evaluation, improving memory
efficiency and speed.
\sphinxhyphen{} It’s important to call \sphinxtitleref{model.eval()} before evaluating the model to set the model to evaluation mode. This is necessary
for models that have layers like dropout or batch normalization that behave differently during training and evaluation.

\end{description}

\end{fulllineitems}

\index{train() (in module insur\_FL\_client)@\spxentry{train()}\spxextra{in module insur\_FL\_client}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_client:insur_FL_client.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_FL\_client.}}\sphinxbfcode{\sphinxupquote{train}}}{\sphinxparam{\DUrole{n,n}{model}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{optimizer}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{criterion}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{train\_loader}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{DataLoader}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{val\_loader}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{DataLoader}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{epochs}\DUrole{o,o}{=}\DUrole{default_value}{10}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Trains the given model using the specified training and validation data loaders, optimizer, and loss function
across a defined number of epochs. Evaluates the model on the validation dataset after each training epoch and
reports the training and validation losses.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{model}{[}torch.nn.Module{]}
\sphinxAtStartPar
The neural network model to be trained.

\sphinxlineitem{optimizer}{[}torch.optim.Optimizer{]}
\sphinxAtStartPar
The optimizer used for adjusting the model parameters based on the computed gradients.

\sphinxlineitem{criterion}{[}torch.nn.Module{]}
\sphinxAtStartPar
The loss function used to evaluate the goodness of the model’s predictions.

\sphinxlineitem{train\_loader}{[}torch.utils.data.DataLoader{]}
\sphinxAtStartPar
DataLoader for the training data, providing batches of data.

\sphinxlineitem{val\_loader}{[}torch.utils.data.DataLoader{]}
\sphinxAtStartPar
DataLoader for the validation data, used to assess the model’s performance.

\sphinxlineitem{epochs}{[}int, optional{]}
\sphinxAtStartPar
The number of complete passes through the training dataset. Defaults to the global variable \sphinxtitleref{EPOCHS}.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{Tuple{[}torch.nn.Module, Dict{[}str, List{[}float{]}{]}{]}}
\sphinxAtStartPar
A tuple containing:
\sphinxhyphen{} The trained model.
\sphinxhyphen{} A dictionary with keys ‘train’ and ‘val’, each mapping to a list of loss values recorded at the end of each epoch.

\end{description}

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{model} \PYG{o}{=} \PYG{n}{MyCustomModel}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{criterion} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{CrossEntropyLoss}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train\PYGZus{}loader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}\PYG{n}{train\PYGZus{}dataset}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{val\PYGZus{}loader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}\PYG{n}{val\PYGZus{}dataset}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{64}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{trained\PYGZus{}model}\PYG{p}{,} \PYG{n}{loss\PYGZus{}stats} \PYG{o}{=} \PYG{n}{train}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{optimizer}\PYG{p}{,} \PYG{n}{criterion}\PYG{p}{,} \PYG{n}{train\PYGZus{}loader}\PYG{p}{,} \PYG{n}{val\PYGZus{}loader}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{loss\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{loss\PYGZus{}stats}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
This function assumes that the \sphinxtitleref{model}, \sphinxtitleref{optimizer}, \sphinxtitleref{criterion}, and data loaders have been initialized

\end{itemize}

\sphinxAtStartPar
before being passed as arguments.
\sphinxhyphen{} The function sets the model in training mode (\sphinxtitleref{model.train()}) at the beginning of each epoch and uses the optimizer
to update model parameters based on the computed gradients.
\sphinxhyphen{} Losses for both training and validation phases are accumulated over each epoch and reported at the end.
\sphinxhyphen{} It’s important to ensure that the device (\sphinxtitleref{cpu} or \sphinxtitleref{cuda}) is correctly configured for the model, data, and criterion
before calling this function.

\end{description}

\end{fulllineitems}


\sphinxstepscope


\section{insur\_FL\_server module}
\label{\detokenize{insur_FL_server:module-insur_FL_server}}\label{\detokenize{insur_FL_server:insur-fl-server-module}}\label{\detokenize{insur_FL_server::doc}}\index{module@\spxentry{module}!insur\_FL\_server@\spxentry{insur\_FL\_server}}\index{insur\_FL\_server@\spxentry{insur\_FL\_server}!module@\spxentry{module}}\index{init\_parameters() (in module insur\_FL\_server)@\spxentry{init\_parameters()}\spxextra{in module insur\_FL\_server}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_server:insur_FL_server.init_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_FL\_server.}}\sphinxbfcode{\sphinxupquote{init\_parameters}}}{}{{ $\rightarrow$ flwr.common.Parameters}}
\pysigstopsignatures
\sphinxAtStartPar
Initialize model parameters using Xavier initialization and return them in a format suitable for FL.
\begin{description}
\sphinxlineitem{Returns:}
\sphinxAtStartPar
fl.common.ParametersRes: The model’s parameters formatted for the Flower FL framework.

\end{description}

\end{fulllineitems}

\index{start\_FL\_server() (in module insur\_FL\_server)@\spxentry{start\_FL\_server()}\spxextra{in module insur\_FL\_server}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_FL_server:insur_FL_server.start_FL_server}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_FL\_server.}}\sphinxbfcode{\sphinxupquote{start\_FL\_server}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Starts a Federated Learning (FL) server with a custom aggregation strategy.

\sphinxAtStartPar
This function initializes the random seed for reproducibility, sets up a custom strategy for aggregating updates from clients participating in the federated learning process, and starts the FL server. The server listens for client connections and orchestrates the federated learning process based on the specified strategy and server configuration.

\sphinxAtStartPar
The aggregation strategy, \sphinxtitleref{LocalUpdatesStrategy}, is configured to require participation from all clients for both training (fit) and evaluation phases. It also initializes the model parameters for the federated learning process.

\sphinxAtStartPar
The server configuration specifies the number of federated learning rounds to be conducted.
\begin{description}
\sphinxlineitem{External Dependencies:}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxtitleref{utils.seed\_torch}: Ensures reproducible results by setting the random seed.

\item {} 
\sphinxAtStartPar
\sphinxtitleref{insur\_secAgg.LocalUpdatesStrategy}: A custom class for the aggregation strategy.

\item {} 
\sphinxAtStartPar
\sphinxtitleref{fl.server.start\_server}: Function to initiate the FL server.

\item {} 
\sphinxAtStartPar
\sphinxtitleref{run\_config.server\_config}: Contains server\sphinxhyphen{}side configurations such as \sphinxtitleref{num\_clients} and \sphinxtitleref{num\_rounds}.

\item {} 
\sphinxAtStartPar
\sphinxtitleref{init\_parameters()}: Function to initialize model parameters for the federated learning process.

\end{itemize}

\sphinxlineitem{Note:}
\sphinxAtStartPar
The server address is hardcoded to “{[}::{]}:8080”, indicating that the server will listen on all available interfaces and the port 8080.

\sphinxlineitem{Example:}\begin{description}
\sphinxlineitem{To start the FL server, simply call this function from your script:}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textasciigrave{}python3 start\_FL\_server()\textasciigrave{}}}

\end{description}

\end{description}

\end{fulllineitems}


\sphinxstepscope


\section{insur\_maskedAgg module}
\label{\detokenize{insur_maskedAgg:module-insur_maskedAgg}}\label{\detokenize{insur_maskedAgg:insur-maskedagg-module}}\label{\detokenize{insur_maskedAgg::doc}}\index{module@\spxentry{module}!insur\_maskedAgg@\spxentry{insur\_maskedAgg}}\index{insur\_maskedAgg@\spxentry{insur\_maskedAgg}!module@\spxentry{module}}\index{LocalUpdatesStrategy (class in insur\_maskedAgg)@\spxentry{LocalUpdatesStrategy}\spxextra{class in insur\_maskedAgg}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_maskedAgg:insur_maskedAgg.LocalUpdatesStrategy}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w,w}{  }}}\sphinxcode{\sphinxupquote{insur\_maskedAgg.}}\sphinxbfcode{\sphinxupquote{LocalUpdatesStrategy}}}{\sphinxparam{\DUrole{o,o}{*}\DUrole{n,n}{args}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{Any}}\sphinxparamcomma \sphinxparam{\DUrole{o,o}{**}\DUrole{n,n}{kwargs}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{Any}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{FedAvg}}

\sphinxAtStartPar
A strategy for federated averaging that considers local updates with optional quantization.

\sphinxAtStartPar
Extends the FedAvg strategy from Flower to aggregate fit results using custom logic, 
including support for quantized model parameter aggregation.
\index{aggregate\_fit() (insur\_maskedAgg.LocalUpdatesStrategy method)@\spxentry{aggregate\_fit()}\spxextra{insur\_maskedAgg.LocalUpdatesStrategy method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_maskedAgg:insur_maskedAgg.LocalUpdatesStrategy.aggregate_fit}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{aggregate\_fit}}}{\sphinxparam{\DUrole{n,n}{server\_round}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{results}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}Tuple\DUrole{p,p}{{[}}flwr.server.client\_proxy.ClientProxy\DUrole{p,p}{,}\DUrole{w,w}{  }flwr.common.FitRes\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{failures}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}Tuple\DUrole{p,p}{{[}}flwr.server.client\_proxy.ClientProxy\DUrole{p,p}{,}\DUrole{w,w}{  }flwr.common.FitRes\DUrole{p,p}{{]}}\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }BaseException\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ Tuple\DUrole{p,p}{{[}}flwr.common.Parameters\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }None\DUrole{p,p}{,}\DUrole{w,w}{  }Dict\DUrole{p,p}{{[}}str\DUrole{p,p}{,}\DUrole{w,w}{  }flwr.common.Scalar\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Aggregate fit results using weighted average with support for quantized parameters.
\begin{description}
\sphinxlineitem{Parameters:}
\sphinxAtStartPar
server\_round (int): The current round of the server.
results (List{[}Tuple{[}ClientProxy, FitRes{]}{]}): The results from clients’ local training.
failures (List{[}Union{[}Tuple{[}ClientProxy, FitRes{]}, BaseException{]}{]}): A list of clients that failed to return results.

\sphinxlineitem{Returns:}
\sphinxAtStartPar
Tuple{[}Optional{[}Parameters{]}, Dict{[}str, float{]}{]}: Aggregated parameters and aggregated metrics.

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{aggregate\_qt() (in module insur\_maskedAgg)@\spxentry{aggregate\_qt()}\spxextra{in module insur\_maskedAgg}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_maskedAgg:insur_maskedAgg.aggregate_qt}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_maskedAgg.}}\sphinxbfcode{\sphinxupquote{aggregate\_qt}}}{\sphinxparam{\DUrole{n,n}{results}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}Tuple\DUrole{p,p}{{[}}flwr.common.NDArrays\DUrole{p,p}{,}\DUrole{w,w}{  }int64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ flwr.common.NDArrays}}
\pysigstopsignatures
\sphinxAtStartPar
Compute the weighted average of quantized model parameters and dequantize the result.
\begin{description}
\sphinxlineitem{Parameters:}
\sphinxAtStartPar
results (List{[}Tuple{[}NDArrays, int{]}{]}): A list of tuples containing quantized model parameters (NDArrays)  and the number of examples (int) used for training.

\sphinxlineitem{Returns:}
\sphinxAtStartPar
NDArrays: The weighted and dequantized average of model parameters.

\end{description}

\end{fulllineitems}

\index{aggregate\_qt2() (in module insur\_maskedAgg)@\spxentry{aggregate\_qt2()}\spxextra{in module insur\_maskedAgg}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_maskedAgg:insur_maskedAgg.aggregate_qt2}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_maskedAgg.}}\sphinxbfcode{\sphinxupquote{aggregate\_qt2}}}{\sphinxparam{\DUrole{n,n}{results}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}Tuple\DUrole{p,p}{{[}}flwr.common.NDArrays\DUrole{p,p}{,}\DUrole{w,w}{  }int\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ flwr.common.NDArrays}}
\pysigstopsignatures
\end{fulllineitems}

\index{aggregate\_weighted\_average() (in module insur\_maskedAgg)@\spxentry{aggregate\_weighted\_average()}\spxextra{in module insur\_maskedAgg}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{insur_maskedAgg:insur_maskedAgg.aggregate_weighted_average}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{insur\_maskedAgg.}}\sphinxbfcode{\sphinxupquote{aggregate\_weighted\_average}}}{\sphinxparam{\DUrole{n,n}{results}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}Tuple\DUrole{p,p}{{[}}flwr.common.NDArrays\DUrole{p,p}{,}\DUrole{w,w}{  }int\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ flwr.common.NDArrays}}
\pysigstopsignatures
\sphinxAtStartPar
Compute the weighted average of model parameters.
\begin{description}
\sphinxlineitem{Parameters:}
\sphinxAtStartPar
results (List{[}Tuple{[}NDArrays, int{]}{]}): A list of tuples where each tuple contains model parameters (NDArrays) and the number of examples (int) used for training.

\sphinxlineitem{Returns:}
\sphinxAtStartPar
NDArrays: The weighted average of model parameters.

\end{description}

\end{fulllineitems}


\sphinxstepscope


\section{prepare\_dataset module}
\label{\detokenize{prepare_dataset:module-prepare_dataset}}\label{\detokenize{prepare_dataset:prepare-dataset-module}}\label{\detokenize{prepare_dataset::doc}}\index{module@\spxentry{module}!prepare\_dataset@\spxentry{prepare\_dataset}}\index{prepare\_dataset@\spxentry{prepare\_dataset}!module@\spxentry{module}}\index{main() (in module prepare\_dataset)@\spxentry{main()}\spxextra{in module prepare\_dataset}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{prepare_dataset:prepare_dataset.main}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{prepare\_dataset.}}\sphinxbfcode{\sphinxupquote{main}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Main execution function that orchestrates the dataset preparation and validation for federated learning.

\end{fulllineitems}

\index{parse\_arguments() (in module prepare\_dataset)@\spxentry{parse\_arguments()}\spxextra{in module prepare\_dataset}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{prepare_dataset:prepare_dataset.parse_arguments}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{prepare\_dataset.}}\sphinxbfcode{\sphinxupquote{parse\_arguments}}}{}{{ $\rightarrow$ Namespace}}
\pysigstopsignatures
\sphinxAtStartPar
Parse command line arguments for federated learning dataset preparation and validation.
\begin{description}
\sphinxlineitem{Returns:}
\sphinxAtStartPar
argparse.Namespace 
An object containing the parsed command line arguments with the following attributes
\begin{quote}
\begin{description}
\sphinxlineitem{num\_agents}{[}int {]}
\sphinxAtStartPar
Specifies the number of agents among which the dataset will be partitioned.

\end{description}
\end{quote}

\end{description}

\end{fulllineitems}

\index{prepare\_and\_validate\_dataset() (in module prepare\_dataset)@\spxentry{prepare\_and\_validate\_dataset()}\spxextra{in module prepare\_dataset}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{prepare_dataset:prepare_dataset.prepare_and_validate_dataset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{prepare\_dataset.}}\sphinxbfcode{\sphinxupquote{prepare\_and\_validate\_dataset}}}{\sphinxparam{\DUrole{n,n}{num\_agents}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Prepares the dataset for federated learning by partitioning it uniformly (by number of records) among the specified number of agents.
Performs validation checks to ensure the integrity of the dataset partitioning.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{num\_agents}{[}int{]}
\sphinxAtStartPar
The number of agents among which the dataset will be partitioned.

\end{description}

\end{description}

\end{fulllineitems}


\sphinxstepscope


\section{skorch\_tuning module}
\label{\detokenize{skorch_tuning:module-skorch_tuning}}\label{\detokenize{skorch_tuning:skorch-tuning-module}}\label{\detokenize{skorch_tuning::doc}}\index{module@\spxentry{module}!skorch\_tuning@\spxentry{skorch\_tuning}}\index{skorch\_tuning@\spxentry{skorch\_tuning}!module@\spxentry{module}}\index{main() (in module skorch\_tuning)@\spxentry{main()}\spxextra{in module skorch\_tuning}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{skorch_tuning:skorch_tuning.main}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{skorch\_tuning.}}\sphinxbfcode{\sphinxupquote{main}}}{}{}
\pysigstopsignatures
\end{fulllineitems}


\sphinxstepscope


\section{utils module}
\label{\detokenize{utils:module-utils}}\label{\detokenize{utils:utils-module}}\label{\detokenize{utils::doc}}\index{module@\spxentry{module}!utils@\spxentry{utils}}\index{utils@\spxentry{utils}!module@\spxentry{module}}\index{create\_df\_sum() (in module utils)@\spxentry{create\_df\_sum()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.create_df_sum}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{create\_df\_sum}}}{\sphinxparam{\DUrole{n,n}{df\_test\_pred}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{factor}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{NUM\_AGENTS}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Create a summary DataFrame aggregating predictions by binned factors.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{df\_test\_pred}{[}pandas.DataFrame{]}
\sphinxAtStartPar
The DataFrame with test predictions.

\sphinxlineitem{factor}{[}str {]}
\sphinxAtStartPar
The factor for binning.

\sphinxlineitem{NUM\_AGENTS}{[}int {]}
\sphinxAtStartPar
The number of agents.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{df\_sum}{[}pandas.DataFrame {]}
\sphinxAtStartPar
The summary DataFrame aggregated by binned factors.

\end{description}

\end{description}

\end{fulllineitems}

\index{create\_df\_test\_pred() (in module utils)@\spxentry{create\_df\_test\_pred()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.create_df_test_pred}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{create\_df\_test\_pred}}}{\sphinxparam{\DUrole{n,n}{df\_test}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{X\_test}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{NUM\_AGENTS}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{global\_model}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{fl\_model}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{agent\_model\_dictionary}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Generate predictions for the test dataset using various models.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{df\_test}{[}pandas.DataFrame{]}
\sphinxAtStartPar
The test dataset.

\sphinxlineitem{X\_test}{[}numpy.ndarray{]}
\sphinxAtStartPar
The features of the test dataset.

\sphinxlineitem{NUM\_AGENTS}{[}int{]}
\sphinxAtStartPar
The number of agents.

\sphinxlineitem{global\_model }
\sphinxAtStartPar
The global model for prediction.

\sphinxlineitem{fl\_model }
\sphinxAtStartPar
The federated learning model for prediction.

\sphinxlineitem{agent\_model\_dictionary}{[}dict{]}
\sphinxAtStartPar
A dictionary containing agent models.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{df\_test}{[}pandas.DataFrame {]}
\sphinxAtStartPar
The test dataset with predictions appended.

\end{description}

\end{description}

\end{fulllineitems}

\index{create\_test\_data() (in module utils)@\spxentry{create\_test\_data()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.create_test_data}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{create\_test\_data}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Create test data for evaluation.

\sphinxAtStartPar
This function loads test data, undummifies categorical variables,
applies scaling to certain features, bins numerical factors, and returns
processed test datasets for evaluation.
\begin{description}
\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{X\_test}{[}pandas.DataFrame{]}
\sphinxAtStartPar
Test features dataset.

\sphinxlineitem{y\_test}{[}pandas.DataFrame{]}
\sphinxAtStartPar
Test labels dataset.

\sphinxlineitem{df\_test}{[}pandas.DataFrame{]}
\sphinxAtStartPar
Processed test dataset.

\end{description}

\end{description}

\end{fulllineitems}

\index{double\_lift\_rebase() (in module utils)@\spxentry{double\_lift\_rebase()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.double_lift_rebase}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{double\_lift\_rebase}}}{\sphinxparam{\DUrole{n,n}{df\_test\_pred}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{model1}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{model2}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Generate a double lift chart comparing the performance of two models.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{df\_test\_pred}{[}pandas.DataFrame{]}
\sphinxAtStartPar
The DataFrame with test predictions.

\sphinxlineitem{model1}{[}str{]}
\sphinxAtStartPar
The name of the first model.

\sphinxlineitem{model2}{[}str {]}
\sphinxAtStartPar
The name of the second model.

\end{description}

\end{description}

\sphinxAtStartPar
Returns:
\sphinxhyphen{} None

\end{fulllineitems}

\index{encode\_and\_scale\_dataframe() (in module utils)@\spxentry{encode\_and\_scale\_dataframe()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.encode_and_scale_dataframe}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{encode\_and\_scale\_dataframe}}}{\sphinxparam{\DUrole{n,n}{df}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Encodes categorical variables and scales numerical features within the DataFrame.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{df}{[}DataFrame {]}
\sphinxAtStartPar
The DataFrame to encode and scale.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{DataFrame }
\sphinxAtStartPar
The encoded and scaled DataFrame.

\sphinxlineitem{MinMaxScaler }
\sphinxAtStartPar
The scaler used for numerical feature scaling.

\end{description}

\end{description}

\sphinxAtStartPar
Usage:
\sphinxcode{\sphinxupquote{\textasciigrave{}
df\_encoded, scaler = encode\_and\_scale\_dataframe(df\_preprocessed)
\textasciigrave{}}}

\end{fulllineitems}

\index{frequency\_conversion() (in module utils)@\spxentry{frequency\_conversion()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.frequency_conversion}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{frequency\_conversion}}}{\sphinxparam{\DUrole{n,n}{FACTOR}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{df}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{freq\_dictionary}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Perform frequency conversion on a DataFrame.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{FACTOR}{[}str {]}
\sphinxAtStartPar
The factor to be converted.

\sphinxlineitem{df}{[}pandas.DataFrame {]}
\sphinxAtStartPar
The DataFrame containing the data.

\sphinxlineitem{freq\_dictionary}{[}dict {]}
\sphinxAtStartPar
A dictionary mapping factor keys to frequency keys.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{df}{[}pandas.DataFrame {]}
\sphinxAtStartPar
The DataFrame with frequency conversion applied.

\end{description}

\end{description}

\end{fulllineitems}

\index{hyperparameter\_counts() (in module utils)@\spxentry{hyperparameter\_counts()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.hyperparameter_counts}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{hyperparameter\_counts}}}{\sphinxparam{\DUrole{n,n}{dataframe}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{hyperparameter}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{x\_label}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{title}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{name}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Plots and saves a bar chart of the value counts for a specified hyperparameter in a given DataFrame.

\sphinxAtStartPar
This function visualizes the distribution of values for a selected hyperparameter within a dataset, 
highlighting the frequency of each unique value. The resulting bar chart is saved to a file.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{dataframe}{[}pandas.DataFrame{]}
\sphinxAtStartPar
The DataFrame containing the data from which to count the hyperparameter values.

\sphinxlineitem{hyperparameter}{[}str{]}
\sphinxAtStartPar
The name of the column in \sphinxtitleref{dataframe} representing the hyperparameter whose distribution is to be plotted.

\sphinxlineitem{x\_label}{[}str{]}
\sphinxAtStartPar
The label for the x\sphinxhyphen{}axis of the plot, typically the name of the hyperparameter.

\sphinxlineitem{title}{[}str{]}
\sphinxAtStartPar
The title of the plot, describing what the plot shows.

\sphinxlineitem{name}{[}str{]}
\sphinxAtStartPar
The filename under which the plot will be saved. The plot is saved in the ‘../results/’ directory.
The ‘.png’ extension is recommended to be included in the \sphinxtitleref{name} for clarity.

\end{description}

\sphinxlineitem{Examples:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}depth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{hyperparameter\PYGZus{}counts}\PYG{p}{(}\PYG{n}{df}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}depth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Model Depth}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Model Depths}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}depth\PYGZus{}distribution.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+go}{\PYGZsh{} This will create and save a bar chart visualizing the frequency of different model depths in the dataset.}
\end{sphinxVerbatim}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
The plot is saved with a white background to ensure readability when viewed on various devices.

\item {} 
\sphinxAtStartPar
Ensure the ‘../results/’ directory exists before calling this function, or adjust the save path accordingly.

\item {} 
\sphinxAtStartPar
The function does not return any value. It directly saves the generated plot to a file.

\end{itemize}

\end{description}

\end{fulllineitems}

\index{load\_individual\_data() (in module utils)@\spxentry{load\_individual\_data()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.load_individual_data}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{load\_individual\_data}}}{\sphinxparam{\DUrole{n,n}{agent\_id}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{include\_val\_in\_train}\DUrole{o,o}{=}\DUrole{default_value}{False}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Loads individual or global datasets as PyTorch TensorDatasets, with an option to include validation data in the training set.

\sphinxAtStartPar
This function dynamically loads training, validation, and test data from CSV files located in a specified directory.
It can load data for a specific agent by ID or global data if the agent ID is set to \sphinxhyphen{}1. There is an option to merge
training and validation datasets for scenarios where validation data should be included in training, e.g., for certain
types of model tuning.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{agent\_id}{[}int{]}
\sphinxAtStartPar
The identifier for the agent’s dataset to load. If set to \sphinxhyphen{}1, global datasets are loaded.

\sphinxlineitem{include\_val\_in\_train}{[}bool, optional{]}
\sphinxAtStartPar
Determines whether validation data is included in the training dataset. Default is False.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{tuple}
\sphinxAtStartPar
A tuple containing the training dataset, validation dataset, test dataset, column names of the training features,
a tensor of test features, and the total exposure calculated from the training (and optionally validation) dataset.

\end{description}

\sphinxlineitem{Examples:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train\PYGZus{}dataset}\PYG{p}{,} \PYG{n}{val\PYGZus{}dataset}\PYG{p}{,} \PYG{n}{test\PYGZus{}dataset}\PYG{p}{,} \PYG{n}{column\PYGZus{}names}\PYG{p}{,} \PYG{n}{test\PYGZus{}features}\PYG{p}{,} \PYG{n}{exposure} \PYG{o}{=} \PYG{n}{load\PYGZus{}individual\PYGZus{}data}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Training dataset size: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{train\PYGZus{}dataset}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}

\index{load\_individual\_skorch\_data() (in module utils)@\spxentry{load\_individual\_skorch\_data()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.load_individual_skorch_data}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{load\_individual\_skorch\_data}}}{\sphinxparam{\DUrole{n,n}{agent\_id}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Loads training, validation, and test datasets for a specified agent or for global model training.

\sphinxAtStartPar
This function reads the datasets from CSV files. If \sphinxtitleref{agent\_id} is \sphinxhyphen{}1, it loads the global datasets.
Otherwise, it loads the agent\sphinxhyphen{}specific datasets based on the provided \sphinxtitleref{agent\_id}.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{agent\_id}{[}int{]}
\sphinxAtStartPar
The identifier for the specific agent’s dataset to load. If set to \sphinxhyphen{}1, the function loads the
global training, validation, and test datasets.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{tuple}
\sphinxAtStartPar
A tuple containing the training features (X\_train\_sc), training labels (y\_tr), validation features
(X\_val\_sc), validation labels (y\_vl), test features (X\_test\_sc), test labels (y\_te), column names
of the training features (X\_column\_names), and the total exposure from the training set (exposure).

\end{description}

\sphinxlineitem{Examples:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}val}\PYG{p}{,} \PYG{n}{y\PYGZus{}val}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{column\PYGZus{}names}\PYG{p}{,} \PYG{n}{exposure} \PYG{o}{=} \PYG{n}{load\PYGZus{}individual\PYGZus{}skorch\PYGZus{}data}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Training data shape: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{X\PYGZus{}train}\PYG{o}{.}\PYG{n}{shape}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}

\index{load\_model() (in module utils)@\spxentry{load\_model()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.load_model}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{load\_model}}}{\sphinxparam{\DUrole{n,n}{agent}\DUrole{o,o}{=}\DUrole{default_value}{\sphinxhyphen{}1}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{num\_features}\DUrole{o,o}{=}\DUrole{default_value}{39}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Load a pre\sphinxhyphen{}trained neural network model for a specific agent.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{agent}{[}int {]}
\sphinxAtStartPar
The ID of the agent whose model to load. Default is \sphinxhyphen{}1.

\sphinxlineitem{num\_features}{[}int {]}
\sphinxAtStartPar
The number of input features for the model. Default is NUM\_FEATURES.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{loaded\_agent\_model}{[}NeuralNetRegressor {]}
\sphinxAtStartPar
The loaded neural network model for the specified agent.

\end{description}

\end{description}

\end{fulllineitems}

\index{lorenz\_curve() (in module utils)@\spxentry{lorenz\_curve()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.lorenz_curve}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{lorenz\_curve}}}{\sphinxparam{\DUrole{n,n}{y\_true}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{y\_pred}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{exposure}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Calculates the Lorenz curve for given true values, predicted values, and exposures.

\sphinxAtStartPar
The Lorenz curve is a graphical representation of the distribution of income or wealth. In this context,
it is used to show the distribution of claims or losses in insurance, ordered by predicted risk. This function
calculates the cumulative percentage of claims and exposures, sorted by the predicted risk.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{y\_true}{[}array\_like{]}
\sphinxAtStartPar
The true values of the claims or losses.

\sphinxlineitem{y\_pred}{[}array\_like{]}
\sphinxAtStartPar
The predicted risk scores associated with each claim or loss.

\sphinxlineitem{exposure}{[}array\_like{]}
\sphinxAtStartPar
The exposure values associated with each observation.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{tuple of numpy.ndarray}
\sphinxAtStartPar
A tuple containing two arrays: the cumulative percentage of exposure and the cumulative percentage of claims,
both sorted by the predicted risk.

\end{description}

\sphinxlineitem{Examples:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{y\PYGZus{}true} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{exposure} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cumulated\PYGZus{}exposure}\PYG{p}{,} \PYG{n}{cumulated\PYGZus{}claims} \PYG{o}{=} \PYG{n}{lorenz\PYGZus{}curve}\PYG{p}{(}\PYG{n}{y\PYGZus{}true}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{exposure}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{cumulated\PYGZus{}exposure}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{cumulated\PYGZus{}claims}\PYG{p}{)}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}

\index{one\_way\_graph\_comparison() (in module utils)@\spxentry{one\_way\_graph\_comparison()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.one_way_graph_comparison}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{one\_way\_graph\_comparison}}}{\sphinxparam{\DUrole{n,n}{factor}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{df\_test\_pred}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{agents\_to\_graph\_list}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{NUM\_AGENTS}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Generate a one\sphinxhyphen{}way graph comparison of actual vs. predicted frequencies by agents.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{factor}{[}str{]}
\sphinxAtStartPar
The factor for binning.

\sphinxlineitem{df\_test\_pred}{[}pandas.DataFrame{]}
\sphinxAtStartPar
The DataFrame with test predictions.

\sphinxlineitem{agents\_to\_graph\_list}{[}list{]}
\sphinxAtStartPar
List of agent indices to include in the graph.

\sphinxlineitem{NUM\_AGENTS}{[}int {]}
\sphinxAtStartPar
The total number of agents.

\end{description}

\end{description}

\sphinxAtStartPar
Returns:
None

\end{fulllineitems}

\index{preprocess\_dataframe() (in module utils)@\spxentry{preprocess\_dataframe()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.preprocess_dataframe}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{preprocess\_dataframe}}}{\sphinxparam{\DUrole{n,n}{df}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Applies preprocessing steps to the dataframe, including shuffling, data type transformations,
and value capping based on specified criteria.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{df}{[}DataFrame {]}
\sphinxAtStartPar
The pandas DataFrame to preprocess.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{DataFrame}
\sphinxAtStartPar
The preprocessed DataFrame.

\end{description}

\end{description}

\sphinxAtStartPar
Usage:
\sphinxcode{\sphinxupquote{\textasciigrave{}
df\_preprocessed = preprocess\_dataframe(df)
\textasciigrave{}}}

\end{fulllineitems}

\index{row\_check() (in module utils)@\spxentry{row\_check()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.row_check}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{row\_check}}}{\sphinxparam{\DUrole{n,n}{agents}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}\DUrole{w,w}{  }\DUrole{o,o}{=}\DUrole{w,w}{  }\DUrole{default_value}{10}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Validates the integrity of the dataset across multiple agents by checking the total number of rows, 
the sum of exposure values, and the sum of claims across training, validation, and test datasets.

\sphinxAtStartPar
This function ensures that the combined dataset from multiple agents, along with the test dataset,
matches expected values for total row count, total exposure, and total claims. These checks are critical for
verifying data integrity and consistency before proceeding with further data analysis or model training.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{agents}{[}int, optional{]}
\sphinxAtStartPar
The number of agents (or partitions) for which training and validation datasets are available.
Defaults to 10.

\end{description}

\sphinxlineitem{Raises:}\begin{description}
\sphinxlineitem{AssertionError}
\sphinxAtStartPar
If the total number of rows, total exposure, or total claims do not match expected values, 
an AssertionError is raised indicating which specific integrity check has failed.

\end{description}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Assumes existence of CSV files in ‘../data/’ following specific naming conventions.

\item {} 
\sphinxAtStartPar
Useful for data preprocessing in machine learning workflows involving multiple sources or agents.

\item {} 
\sphinxAtStartPar
‘Exposure’ and ‘0’ are assumed to be column names in the respective CSV files for exposure and claims.

\end{itemize}

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{row\PYGZus{}check}\PYG{p}{(}\PYG{n}{agents}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{g+go}{\PYGZsh{} Checks datasets for 5 agents, along with the test dataset, and prints the status of each check.}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}

\index{seed\_torch() (in module utils)@\spxentry{seed\_torch()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.seed_torch}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{seed\_torch}}}{\sphinxparam{\DUrole{n,n}{seed}\DUrole{o,o}{=}\DUrole{default_value}{300}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Seeds the random number generators of PyTorch, NumPy, and Python’s \sphinxtitleref{random} module to ensure
reproducibility of results across runs when using PyTorch for deep learning experiments.

\sphinxAtStartPar
This function sets the seed for PyTorch (both CPU and CUDA), NumPy, and the Python \sphinxtitleref{random} module,
enabling CuDNN benchmarking and deterministic algorithms. It is crucial for experiments requiring
reproducibility, like model performance comparisons. Note that enabling CuDNN benchmarking and
deterministic operations may impact performance and limit certain optimizations.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{seed}{[}int, optional{]}
\sphinxAtStartPar
The seed value to use for all random number generators. The default value is \sphinxtitleref{SEED}, which
should be defined beforehand.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{None}
\sphinxAtStartPar
This function does not return a value but sets the random seed for various libraries.

\end{description}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
When using multiple GPUs, \sphinxtitleref{th.cuda.manual\_seed\_all(seed)} ensures all GPUs are seeded,

\end{itemize}

\sphinxAtStartPar
crucial for reproducibility in multi\sphinxhyphen{}GPU setups.

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{SEED} \PYG{o}{=} \PYG{l+m+mi}{42}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{seed\PYGZus{}torch}\PYG{p}{(}\PYG{n}{SEED}\PYG{p}{)}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}

\index{split\_data() (in module utils)@\spxentry{split\_data()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.split_data}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{split\_data}}}{\sphinxparam{\DUrole{n,n}{df\_encoded}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Splits the encoded DataFrame into training, validation, and test sets.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{df\_encoded}{[}DataFrame {]}
\sphinxAtStartPar
The encoded DataFrame from which to split the data.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{tuple}
\sphinxAtStartPar
Contains training, validation, and test sets (X\_train, X\_val, X\_test, y\_train, y\_val, y\_test).

\end{description}

\end{description}

\sphinxAtStartPar
Usage:
\sphinxcode{\sphinxupquote{\textasciigrave{}
X\_train, X\_val, X\_test, y\_train, y\_val, y\_test = split\_data(df\_encoded)
\textasciigrave{}}}

\end{fulllineitems}

\index{training\_loss\_curve() (in module utils)@\spxentry{training\_loss\_curve()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.training_loss_curve}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{training\_loss\_curve}}}{\sphinxparam{\DUrole{n,n}{estimator}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{agent\_id}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Plots the training and validation loss curves along with the percentage of Poisson Deviance Explained (PDE).
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{estimator}{[}object{]}
\sphinxAtStartPar
The trained model or estimator that contains the training history. It is expected
to have a ‘history’ attribute that is a NumPy array or a similar structure with
‘train\_loss’, ‘valid\_loss’, and ‘weighted\_PDE\_best’ columns.

\sphinxlineitem{agent\_id}{[}int or str{]}
\sphinxAtStartPar
Identifier for the agent. Used for titling the plot and naming the saved figure file.

\end{description}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
This function saves the generated plot as a PNG file in a directory named after the agent.

\item {} 
\sphinxAtStartPar
Ensure the directory ‘../ag\_\{agent\_id\}/’ exists or adjust the save path accordingly.

\item {} 
\sphinxAtStartPar
The function uses matplotlib for plotting and requires this library to be installed.

\end{itemize}

\end{description}

\end{fulllineitems}

\index{undummify() (in module utils)@\spxentry{undummify()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.undummify}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{undummify}}}{\sphinxparam{\DUrole{n,n}{df}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{prefix\_sep}\DUrole{o,o}{=}\DUrole{default_value}{\textquotesingle{}\_\textquotesingle{}}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Reverse one\sphinxhyphen{}hot encoding (dummy variables) in a DataFrame.
\begin{description}
\sphinxlineitem{Parameters:}\begin{itemize}
\item {} 
\sphinxAtStartPar
df (pandas.DataFrame): The DataFrame containing dummy variables.

\item {} 
\sphinxAtStartPar
prefix\_sep (str, optional): Separator used in column prefixes. Default is “\_”.

\end{itemize}

\sphinxlineitem{Returns:}
\sphinxAtStartPar
undummified\_df (pandas.DataFrame): The DataFrame with one\sphinxhyphen{}hot encoding reversed.

\end{description}

\end{fulllineitems}

\index{uniform\_partitions() (in module utils)@\spxentry{uniform\_partitions()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.uniform_partitions}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{uniform\_partitions}}}{\sphinxparam{\DUrole{n,n}{agents}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}\DUrole{w,w}{  }\DUrole{o,o}{=}\DUrole{w,w}{  }\DUrole{default_value}{10}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{num\_features}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int\DUrole{w,w}{  }\DUrole{p,p}{|}\DUrole{w,w}{  }None}\DUrole{w,w}{  }\DUrole{o,o}{=}\DUrole{w,w}{  }\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Splits and saves the dataset into uniform partitions for a specified number of agents.

\sphinxAtStartPar
This function loads a dataset via a previously defined \sphinxtitleref{upload\_dataset} function, then partitions
the training and validation datasets uniformly across the specified number of agents. Each partition
is saved to CSV files, containing both features and labels for each agent’s training and validation datasets.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{agents}{[}int, optional{]}
\sphinxAtStartPar
The number of agents to split the dataset into. Defaults to 10.

\sphinxlineitem{num\_features}{[}int, optional{]}
\sphinxAtStartPar
The number of features in the dataset. Automatically inferred if not specified.

\end{description}

\sphinxlineitem{Notes:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Requires \sphinxtitleref{upload\_dataset} and \sphinxtitleref{seed\_torch} to be defined and accessible within the scope.

\item {} 
\sphinxAtStartPar
Saves partitioned data files in the ‘../data/’ directory.

\end{itemize}

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{uniform\PYGZus{}partitions}\PYG{p}{(}\PYG{n}{agents}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{g+go}{Creates and saves 5 sets of training and validation data for 5 agents, storing them in \PYGZsq{}../data/\PYGZsq{}.}
\end{sphinxVerbatim}

\sphinxlineitem{Raises:}\begin{description}
\sphinxlineitem{FileNotFoundError}
\sphinxAtStartPar
If the ‘../data/’ directory does not exist or cannot be accessed.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{None}
\sphinxAtStartPar
The function does not return a value but saves partitioned datasets to disk.

\end{description}

\end{description}

\end{fulllineitems}

\index{upload\_dataset() (in module utils)@\spxentry{upload\_dataset()}\spxextra{in module utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils:utils.upload_dataset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils.}}\sphinxbfcode{\sphinxupquote{upload\_dataset}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Uploads, preprocesses, encodes, scales, and splits the dataset into training, validation, and test sets.

\sphinxAtStartPar
Assumes the existence of a global \sphinxtitleref{DATA\_PATH} variable pointing to the dataset’s location and a \sphinxtitleref{SEED} for reproducibility.
\begin{description}
\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{tuple}
\sphinxAtStartPar
Contains the training, validation, and test sets, feature names, and the scaler.

\end{description}

\end{description}

\sphinxAtStartPar
Usage:
\sphinxcode{\sphinxupquote{\textasciigrave{}
X\_train, X\_val, X\_test, y\_train, y\_val, y\_test, feature\_names, scaler = upload\_dataset()
\textasciigrave{}}}

\end{fulllineitems}


\sphinxAtStartPar
\#   :exclude\sphinxhyphen{}members: load\_model,  frequency\_conversion, undummify, create\_test\_data, create\_df\_test\_pred, create\_df\_sum, one\_way\_graph\_comparison, double\_lift\_rebase

\sphinxstepscope


\section{utils\_quantisation module}
\label{\detokenize{utils_quantisation:module-utils_quantisation}}\label{\detokenize{utils_quantisation:utils-quantisation-module}}\label{\detokenize{utils_quantisation::doc}}\index{module@\spxentry{module}!utils\_quantisation@\spxentry{utils\_quantisation}}\index{utils\_quantisation@\spxentry{utils\_quantisation}!module@\spxentry{module}}\index{add\_mod() (in module utils\_quantisation)@\spxentry{add\_mod()}\spxextra{in module utils\_quantisation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils_quantisation:utils_quantisation.add_mod}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils\_quantisation.}}\sphinxbfcode{\sphinxupquote{add\_mod}}}{\sphinxparam{\DUrole{n,n}{x}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}int64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{y}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}int64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}}{{ $\rightarrow$ ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}int64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs modular addition on two integer numpy arrays with a predefined modulus.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{x}{[}NDArrayInt{]}
\sphinxAtStartPar
The first integer numpy array.

\sphinxlineitem{y}{[}NDArrayInt{]}
\sphinxAtStartPar
The second integer numpy array.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{NDArrayInt}
\sphinxAtStartPar
The result of modular addition of \sphinxtitleref{x} and \sphinxtitleref{y}.

\end{description}

\end{description}

\end{fulllineitems}

\index{dequantize() (in module utils\_quantisation)@\spxentry{dequantize()}\spxextra{in module utils\_quantisation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils_quantisation:utils_quantisation.dequantize}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils\_quantisation.}}\sphinxbfcode{\sphinxupquote{dequantize}}}{\sphinxparam{\DUrole{n,n}{quantized\_parameters}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}int64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{clipping\_range}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{float}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{target\_range}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int64}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{ag\_no}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}}}{{ $\rightarrow$ List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}float64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dequantizes a list of integer numpy arrays back into float numpy arrays, adjusting for a specified clipping range.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{quantized\_parameters}{[}List{[}NDArrayInt{]}{]}
\sphinxAtStartPar
The list of quantized integer numpy arrays to be dequantized.

\sphinxlineitem{clipping\_range}{[}float{]}
\sphinxAtStartPar
The clipping range to adjust the dequantized values within.

\sphinxlineitem{target\_range}{[}np.int64{]}
\sphinxAtStartPar
The original target range used for quantization.

\sphinxlineitem{ag\_no}{[}int{]}
\sphinxAtStartPar
A factor to adjust the dequantization process, potentially representing the number of aggregating agents.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{List{[}NDArrayFloat{]}}
\sphinxAtStartPar
The list of dequantized float numpy arrays.

\end{description}

\end{description}

\end{fulllineitems}

\index{dequantize\_mean() (in module utils\_quantisation)@\spxentry{dequantize\_mean()}\spxextra{in module utils\_quantisation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils_quantisation:utils_quantisation.dequantize_mean}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils\_quantisation.}}\sphinxbfcode{\sphinxupquote{dequantize\_mean}}}{\sphinxparam{\DUrole{n,n}{quantized\_parameters}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}int64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{clipping\_range}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{float}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{target\_range}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int64}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{ag\_no}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}}}{{ $\rightarrow$ List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}float64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dequantizes integer Numpy arrays back to float Numpy arrays with an adjusted range based on
clipping range, target range, and the number of agents.

\sphinxAtStartPar
This function reverses the quantization process applied to the original float arrays, taking into account
the clipping range, target range, and an adjustment factor derived from the number of agents. It’s used
to restore the approximate original float values from their quantized integer representations, applying
an average based on the number of agents to adjust the quantization scale.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{quantized\_parameters}{[}List{[}NDArrayInt{]}{]}
\sphinxAtStartPar
A list of quantized integer Numpy arrays to be dequantized.

\sphinxlineitem{clipping\_range}{[}float{]}
\sphinxAtStartPar
The maximum absolute value allowed in the original float arrays.

\sphinxlineitem{target\_range}{[}np.int64{]}
\sphinxAtStartPar
The target range used during the quantization process.

\sphinxlineitem{ag\_no}{[}int{]}
\sphinxAtStartPar
The number of agents involved, which affects the dequantization scale.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{List{[}NDArrayFloat{]}}
\sphinxAtStartPar
A list of dequantized float Numpy arrays, with values approximately restored to
their original scale and centered around the original clipping range.

\end{description}

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{quantized\PYGZus{}parameters} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{2000}\PYG{p}{]}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{int64}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{clipping\PYGZus{}range} \PYG{o}{=} \PYG{l+m+mf}{1.0}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{target\PYGZus{}range} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ag\PYGZus{}no} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{dequantized} \PYG{o}{=} \PYG{n}{dequantize\PYGZus{}mean}\PYG{p}{(}\PYG{n}{quantized\PYGZus{}parameters}\PYG{p}{,} \PYG{n}{clipping\PYGZus{}range}\PYG{p}{,} \PYG{n}{target\PYGZus{}range}\PYG{p}{,} \PYG{n}{ag\PYGZus{}no}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{dequantized}\PYG{p}{)}
\PYG{g+go}{[array([\PYGZhy{}0.2, 0.2, \PYGZhy{}0.4], dtype=float64)]}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}

\index{quantize() (in module utils\_quantisation)@\spxentry{quantize()}\spxextra{in module utils\_quantisation}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils_quantisation:utils_quantisation.quantize}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils\_quantisation.}}\sphinxbfcode{\sphinxupquote{quantize}}}{\sphinxparam{\DUrole{n,n}{parameters}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}float64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{clipping\_range}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{float}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{target\_range}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int64}}}{{ $\rightarrow$ List\DUrole{p,p}{{[}}ndarray\DUrole{p,p}{{[}}Any\DUrole{p,p}{,}\DUrole{w,w}{  }dtype\DUrole{p,p}{{[}}int64\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}\DUrole{p,p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Quantizes a list of float numpy arrays into integer numpy arrays within a specified target range.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{parameters}{[}List{[}NDArrayFloat{]}{]}
\sphinxAtStartPar
The list of float numpy arrays to be quantized.

\sphinxlineitem{clipping\_range}{[}float{]}
\sphinxAtStartPar
The range within which values are clipped before quantization.

\sphinxlineitem{target\_range}{[}np.int64{]}
\sphinxAtStartPar
The integer range to quantize the values into.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{List{[}NDArrayInt{]}}
\sphinxAtStartPar
The list of quantized integer numpy arrays.

\end{description}

\end{description}

\end{fulllineitems}


\sphinxstepscope


\section{utils\_smpc module}
\label{\detokenize{utils_smpc:module-utils_smpc}}\label{\detokenize{utils_smpc:utils-smpc-module}}\label{\detokenize{utils_smpc::doc}}\index{module@\spxentry{module}!utils\_smpc@\spxentry{utils\_smpc}}\index{utils\_smpc@\spxentry{utils\_smpc}!module@\spxentry{module}}\index{calc\_noise() (in module utils\_smpc)@\spxentry{calc\_noise()}\spxextra{in module utils\_smpc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils_smpc:utils_smpc.calc_noise}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils\_smpc.}}\sphinxbfcode{\sphinxupquote{calc\_noise}}}{\sphinxparam{\DUrole{n,n}{file\_path}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{str}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{agent\_number}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}}}{{ $\rightarrow$ dict}}
\pysigstopsignatures
\sphinxAtStartPar
Calculate noise based on seeds stored in a CSV file for a specific agent.

\sphinxAtStartPar
This function reads a CSV file containing seeds for each collaborator by round,
generates noise vectors for each round, and applies quantization if enabled in 
the run configuration. It differentiates the noise for the specific agent from others.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{file\_path}{[}str{]}
\sphinxAtStartPar
The path to the CSV file containing the seeds.

\sphinxlineitem{agent\_number}{[}int{]}
\sphinxAtStartPar
The index of the current agent for whom the noise is being calculated.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{dict}
\sphinxAtStartPar
A dictionary with round numbers as keys and noise vectors as values.

\end{description}

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{noises} \PYG{o}{=} \PYG{n}{calc\PYGZus{}noise}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{path/to/seeds.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{noises}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Noise vector for round 1}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}

\index{calc\_noise\_zero() (in module utils\_smpc)@\spxentry{calc\_noise\_zero()}\spxextra{in module utils\_smpc}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{utils_smpc:utils_smpc.calc_noise_zero}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{utils\_smpc.}}\sphinxbfcode{\sphinxupquote{calc\_noise\_zero}}}{\sphinxparam{\DUrole{n,n}{file\_path}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{str}}\sphinxparamcomma \sphinxparam{\DUrole{n,n}{agent\_number}\DUrole{p,p}{:}\DUrole{w,w}{  }\DUrole{n,n}{int}}}{{ $\rightarrow$ dict}}
\pysigstopsignatures
\sphinxAtStartPar
Generate a dictionary of zero noise vectors for each round based on the CSV file.
\begin{description}
\sphinxlineitem{Parameters:}\begin{description}
\sphinxlineitem{file\_path}{[}str{]}
\sphinxAtStartPar
The path to the CSV file containing the seeds.

\sphinxlineitem{agent\_number}{[}int{]}
\sphinxAtStartPar
The agent number, unused in this function but maintained for API consistency.

\end{description}

\sphinxlineitem{Returns:}\begin{description}
\sphinxlineitem{dict}
\sphinxAtStartPar
A dictionary with round numbers as keys and zero noise vectors as values.

\end{description}

\sphinxlineitem{Example:}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{zero\PYGZus{}noises} \PYG{o}{=} \PYG{n}{calc\PYGZus{}noise\PYGZus{}zero}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{path/to/seeds.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{zero\PYGZus{}noises}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Zero noise vector for round 1}
\end{sphinxVerbatim}

\end{description}

\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{a}
\item\relax\sphinxstyleindexentry{architecture}\sphinxstyleindexpageref{architecture:\detokenize{module-architecture}}
\indexspace
\bigletter{i}
\item\relax\sphinxstyleindexentry{insur\_FL\_client}\sphinxstyleindexpageref{insur_FL_client:\detokenize{module-insur_FL_client}}
\item\relax\sphinxstyleindexentry{insur\_FL\_server}\sphinxstyleindexpageref{insur_FL_server:\detokenize{module-insur_FL_server}}
\item\relax\sphinxstyleindexentry{insur\_maskedAgg}\sphinxstyleindexpageref{insur_maskedAgg:\detokenize{module-insur_maskedAgg}}
\indexspace
\bigletter{p}
\item\relax\sphinxstyleindexentry{prepare\_dataset}\sphinxstyleindexpageref{prepare_dataset:\detokenize{module-prepare_dataset}}
\indexspace
\bigletter{s}
\item\relax\sphinxstyleindexentry{skorch\_tuning}\sphinxstyleindexpageref{skorch_tuning:\detokenize{module-skorch_tuning}}
\indexspace
\bigletter{u}
\item\relax\sphinxstyleindexentry{utils}\sphinxstyleindexpageref{utils:\detokenize{module-utils}}
\item\relax\sphinxstyleindexentry{utils\_quantisation}\sphinxstyleindexpageref{utils_quantisation:\detokenize{module-utils_quantisation}}
\item\relax\sphinxstyleindexentry{utils\_smpc}\sphinxstyleindexpageref{utils_smpc:\detokenize{module-utils_smpc}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}